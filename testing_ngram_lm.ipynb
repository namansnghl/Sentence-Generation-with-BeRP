{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Implement an n-gram Language Model on BeRP Dataset\n",
    "----\n",
    "\n",
    "Your language model:\n",
    "- *must* work for the unigram, bigram, and trigram cases (5 points are allocated to an experiment involving larger values of `n`)\n",
    "    - hint: try to implement the bigram case as a generalized \"n greater than 1\" case\n",
    "- should be *token agnostic* (this means that if we give the model text tokenized as single characters, it will function as a character language model and if we give the model text tokenized as \"words\" (or \"traditionally\"), then it will function as a language model with those tokens)\n",
    "- will use Laplace smoothing\n",
    "- will replace all tokens that occur only once with `<UNK>` at train time\n",
    "    - do not add `<UNK>` to your vocabulary if no tokens in the training data occur only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lm_model as lm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'am', 'ham', 'am', '</s>'], ['<s>', '</s>']]\n",
      "[['<s>', 'sam', '</s>'], ['<s>', 'i', 'am', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# test the language model (unit tests)\n",
    "import test_minitrainingprovided as test\n",
    "\n",
    "# passing all these tests is a good indication that your model\n",
    "# is correct. They are *not a guarantee*, so make sure to look\n",
    "# at the tests and the cases that they cover. (we'll be testing\n",
    "# your model against all of the testing data in addition).\n",
    "\n",
    "# autograder points in gradescope are assigned SIXTY points\n",
    "# this is essentially 60 points for correctly implementing your\n",
    "# underlying model\n",
    "# there are an additional 10 points manually graded for the correctness\n",
    "# parts of your sentence generation\n",
    "\n",
    "# make sure all training files are in a \"training_files\" directory \n",
    "# that is in the same directory as this notebook\n",
    "\n",
    "unittest = test.TestMiniTraining()\n",
    "unittest.test_createunigrammodellaplace()\n",
    "unittest.test_createbigrammodellaplace()\n",
    "unittest.test_unigramlaplace()\n",
    "unittest.test_unigramunknownslaplace()\n",
    "unittest.test_bigramlaplace()\n",
    "unittest.test_bigramunknownslaplace()\n",
    "# produces output\n",
    "unittest.test_generateunigramconcludes()\n",
    "# produces output\n",
    "unittest.test_generatebigramconcludes()\n",
    "\n",
    "unittest.test_onlyunknownsgenerationandscoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'start', 'over', '</s>']\n",
      "['<s>', \"i'd\", 'like', 'some', 'uh', 'i', 'will', 'spend', 'more', 'than', 'one', 'hundred', 'dollars', '</s>']\n",
      "['<s>', 'asian', 'lunch', 'in', 'berkeley', '</s>']\n",
      "['<s>', \"let's\", 'start', 'over', '</s>']\n",
      "['<s>', 'looking', 'for', 'dinner', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# instantiate a bigram language model, train it, and generate ten sentences\n",
    "# make sure your output is nicely formatted!\n",
    "ngram = 2\n",
    "training_file_path = \"training_files/berp-training.txt\"\n",
    "# optional parameter tells the tokenize function how to tokenize\n",
    "by_char = False\n",
    "data = lm.read_file(training_file_path)\n",
    "tokens = lm.tokenize(data, ngram, by_char=by_char)\n",
    "\n",
    "model = lm.LanguageModel(ngram, training_file=training_file_path)\n",
    "model.train(tokens)\n",
    "print(*model.generate(5), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average =  4.962082362726267e-05\n",
      "Standard Deviation =  0.000286735365135695\n"
     ]
    }
   ],
   "source": [
    "# evaluate your bigram model on the test data\n",
    "# score each line in the test data individually, then calculate the average score\n",
    "# you need not re-train your model\n",
    "test_path = \"testing_files/berp-test.txt\"\n",
    "test_data = lm.read_file(test_path)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for sentence in test_data:\n",
    "    tok = lm.tokenize_line(sentence, ngram, by_char)\n",
    "    scores.append(model.score(tok))\n",
    "\n",
    "\n",
    "# Print out the mean score and standard deviation\n",
    "# for words-as-tokens, these values should be\n",
    "# ~4.9 * 10^-5 and 0.000285\n",
    "print(\"Average = \", sum(scores)/len(scores))\n",
    "print(\"Standard Deviation = \",statistics.stdev(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if you can train your model on the data you found for your first homework\n",
    "n = 30\n",
    "model = lm.LanguageModel(n, r'./training_files/cancer_small_data.txt')\n",
    "model.train()\n",
    "# what is the maximum value of n <= 10 that you can train a model *in your programming environment* in a reasonable amount of time? (less than 3 - 5 minutes)\n",
    "# Tried with 100 and it worked. Took 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE 1\n",
      "Original Sentence Length  4677\n",
      "First 100 Words:\n",
      "<UNK> the frequency and clinical features of lungnodules in IgG4 related disease IgG4RD patients as an insight for help with the diagnosis of lung nodulesMethods A retrospective study was carried out in West China Hospital Sichuan University from January toDecember patients with definite IgG4RD were <UNK> Fifty of patients with definite\n",
      "********************\n",
      "\n",
      "SENTENCE 2\n",
      "Original Sentence Length  3327\n",
      "First 100 Words:\n",
      "Human transcription factor and protein kinase gene fusions in human <UNK> <UNK> <UNK> G <UNK> <UNK> <UNK> gene fusions are estimated to account for upto of cancer morbidity Recently <UNK> studies have established oncofusions throughout all tissue types However the functional implications of the identified oncofusions have often not been investigated\n",
      "********************\n",
      "\n",
      "SENTENCE 3\n",
      "Original Sentence Length  3095\n",
      "First 100 Words:\n",
      "This study aimed to investigate serum matrix metalloproteinase MMP2 and MMP9levels in patients with papillary thyroid carcinoma PTCMethods <UNK> patients with PTC undergoing ultrasoundguided radiofrequency ablationRFA and controls were included Serum MMP2 and MMP9 levels were determined <UNK> immunosorbent assay before and after surgery Potential affecting factors wereevaluated by logistic regression\n",
      "********************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate three sentences with this model\n",
    "cnt = 1\n",
    "for _ in range(3):\n",
    "    print(f\"SENTENCE {cnt}\")\n",
    "    cnt+=1\n",
    "    sentence1 = model.generate_sentence()\n",
    "    print(\"Original Sentence Length \", len(sentence1))\n",
    "    print(\"First 100 Words:\")\n",
    "    print(\" \".join(sentence1[n-1:n+50]))\n",
    "    print(\"*\"*20)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the corresponding function and evaluate the perplexity of your model on the first 20 lines in the test data for values of `n` from 1 to 3. Perplexity should be individually calculated for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "Ngram model: 1\n",
      "<s> restaurant </s>\n",
      "<s> me <UNK> restaurant restaurant please like list i night <UNK> dollars some for could about is icsi dinner know <UNK> have any would food </s>\n",
      "<s> dollars to uh <UNK> </s>\n",
      "<s> i'd </s>\n",
      "<s> don't </s>\n",
      "<s> are there i give for computer restaurant on </s>\n",
      "<s> distance i'd than give </s>\n",
      "<s> to pizza want have <UNK> go food would is restaurant <UNK> best know information list travel is <UNK> restaurants the </s>\n",
      "<s> </s>\n",
      "<s> <UNK> <UNK> <UNK> more a are expensive do <UNK> <UNK> <UNK> i <UNK> cost best restaurant about </s>\n",
      "############ Mean Perplexity 41.94660860789063 ############\n",
      "\n",
      "********\n",
      "Ngram model: 2\n",
      "<s> do you at <UNK> not have <UNK> for breakfast any of the restaurant </s>\n",
      "<s> i'd like to the best <UNK> restaurant </s>\n",
      "<s> i'd like to eat <UNK> information on <UNK> how about <UNK> </s>\n",
      "<s> what is <UNK> from the list the best <UNK> </s>\n",
      "<s> i'd like to eat on <UNK> not expensive </s>\n",
      "<s> the restaurants <UNK> <UNK> </s>\n",
      "<s> uh i'm willing to have to travel seven minutes <UNK> restaurant </s>\n",
      "<s> three dollars </s>\n",
      "<s> i'm <UNK> not be <UNK> </s>\n",
      "<s> and <UNK> </s>\n",
      "############ Mean Perplexity 21.43014616349896 ############\n",
      "\n",
      "********\n",
      "Ngram model: 3\n",
      "<s> <s> do you know any pizza <UNK> </s> </s>\n",
      "<s> <s> i like to <UNK> ten miles </s> </s>\n",
      "<s> <s> <UNK> <UNK> <UNK> </s> </s>\n",
      "<s> <s> it <UNK> <UNK> <UNK> </s> </s>\n",
      "<s> <s> show me a menu </s> </s>\n",
      "<s> <s> what is the international <UNK> cafe from the international computer <UNK> <UNK> for some <UNK> <UNK> </s> </s>\n",
      "<s> <s> i want restaurants anywhere </s> </s>\n",
      "<s> <s> dinner </s> </s>\n",
      "<s> <s> i'm <UNK> <UNK> about three and <UNK> </s> </s>\n",
      "<s> <s> and <UNK> dollars to spend less than ten <UNK> </s> </s>\n",
      "############ Mean Perplexity 17.19274413541194 ############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = \"testing_files/berp-test.txt\"\n",
    "test_data = lm.read_file(test_path)\n",
    "\n",
    "for ngram in range(1, 4):\n",
    "    print(\"********\")\n",
    "    print(\"Ngram model:\", ngram)\n",
    "    model = lm.LanguageModel(ngram,training_file=test_path)\n",
    "    model.train()\n",
    "    sentences = model.generate(10)\n",
    "    perplexities = [model.perplexity(sentence) for sentence in sentences]\n",
    "    print(*[\" \".join(sentence) for sentence in sentences], sep='\\n')\n",
    "    print(f\"############ Mean Perplexity {sum(perplexities)/len(perplexities)} ############\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
